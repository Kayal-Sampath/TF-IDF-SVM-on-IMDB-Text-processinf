{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text processing : IMDB .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB-hAwjrdRM_"
      },
      "source": [
        "## Data set loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6VpgzXiRGa3"
      },
      "source": [
        "**Dataset Description:**\n",
        "\n",
        "Dataset used: IMDB Dataset\n",
        "\n",
        "Dataset description: \n",
        "1. IMDB dataset having 50K movie reviews for natural\n",
        "language processing or Text analytics.\n",
        "2. Dataset is of binary sentiment classification\n",
        "3.  Total instances - 50,000 (sample : 21)\n",
        "4.  To classify the reviews as either positive or negative using either classification algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLWCtCzfcbZC",
        "outputId": "ff25625c-a571-4eaa-87a7-f74fcb284691"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Xkn7xlXvjg"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9cILDFqWPWT",
        "outputId": "6636855f-609d-4a12-8baa-f8c78f7d1951"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525lc4mUXrUC"
      },
      "source": [
        "**Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "RcZraoh5WVW-",
        "outputId": "a29534a2-9543-49e9-fed7-2703d0160d10"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/SACE/IMDB_sample.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade. T...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this film,it was the first film i h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film! It must have been up against so...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>After the success of Die Hard and it's sequels...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>I had the terrible misfortune of having to vie...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review sentiment\n",
              "0   One of the other reviewers has mentioned that ...  positive\n",
              "1   A wonderful little production. <br /><br />The...  positive\n",
              "2   I thought this was a wonderful way to spend ti...  positive\n",
              "3   Basically there's a family where a little boy ...  negative\n",
              "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5   Probably my all-time favorite movie, a story o...  positive\n",
              "6   I sure would like to see a resurrection of a u...  positive\n",
              "7   This show was an amazing, fresh & innovative i...  negative\n",
              "8   Encouraged by the positive comments about this...  negative\n",
              "9   If you like original gut wrenching laughter yo...  positive\n",
              "10  Phil the Alien is one of those quirky films wh...  negative\n",
              "11  I saw this movie when I was about 12 when it c...  negative\n",
              "12  So im not a big fan of Boll's work but then ag...  negative\n",
              "13  The cast played Shakespeare.<br /><br />Shakes...  negative\n",
              "14  This a fantastic movie of three prisoners who ...  positive\n",
              "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
              "16  Some films just simply should not be remade. T...  positive\n",
              "17  This movie made it into one of my top 10 most ...  negative\n",
              "18  I remember this film,it was the first film i h...  positive\n",
              "19  An awful film! It must have been up against so...  negative\n",
              "20  After the success of Die Hard and it's sequels...  positive\n",
              "21  I had the terrible misfortune of having to vie...  negative"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB35JmQPsEO1",
        "outputId": "650439fa-d54d-4655-ea02-b268047936f7"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    11\n",
              "negative    11\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbsK4gHKf2Q3"
      },
      "source": [
        "## Data wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgeQVCS4gXNo"
      },
      "source": [
        "1. Lower case\n",
        "2. URL removal\n",
        "3. Number removal\n",
        "4. Tokenizing : Regexp\n",
        "5. Stemming : stemmer\n",
        "6. Lemmatizer : lemmatize\n",
        "7. Stop word removal : stopwords & length less than 2 (time : )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrqfIkQ3gV94"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    #rem_punc = re.sub(r'[^\\w\\s]', '', rem_num)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)  \n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "df['cleanText']=df['review'].map(lambda s:preprocess(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5aSnPGjWiLIr",
        "outputId": "5e2db2d0-9b95-489a-f68b-5676bab1144d"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleanText</th>\n",
              "      <th>sentiment_le</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>one reviewers mentioned watching episode hooke...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically family little boy jake thinks zombie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter mattei love time money visually stunnin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... sentiment_le\n",
              "0  One of the other reviewers has mentioned that ...  ...            1\n",
              "1  A wonderful little production. <br /><br />The...  ...            1\n",
              "2  I thought this was a wonderful way to spend ti...  ...            1\n",
              "3  Basically there's a family where a little boy ...  ...            0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...            1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWBb51JBkaLZ",
        "outputId": "8c1eb7f1-f3bd-4429-d64e-80e7c659633b"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    print(\"Sentence in lower case:\\n\", sentence)\n",
        "    print(\"_________________________________________________\")\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    print(\"After removing urls:\\n\", rem_url)\n",
        "    print(\"_________________________________________________\")\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    print(\"After removing numbers:\\n\", rem_num)\n",
        "    print(\"_________________________________________________\")\n",
        "    #rem_punc = re.sub(r'[^\\w\\s]', '', rem_num)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)  \n",
        "    print(\"After tokenization:\\n\",tokens)\n",
        "    print(\"_________________________________________________\")\n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    print(\"After removing stop words:\\n\", filtered_words)\n",
        "    print(\"_________________________________________________\")\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    print(\"After stemming:\\n\", stem_words)\n",
        "    print(\"_________________________________________________\")\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "    print(\"After lemmatization:\\n\", lemma_words)\n",
        "    print(\"_________________________________________________\")\n",
        "    return \" \".join(lemma_words)\n",
        "\n",
        "sentence = \"If you like 100 original gut wrenching laughter you will leaves like this movie. https:www.google.com \\ .<br /><br />Great Camp!!!\"\n",
        "print(sentence)\n",
        "clean_sent = preprocess(sentence)\n",
        "print(clean_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you like 100 original gut wrenching laughter you will leaves like this movie. https:www.google.com \\ .<br /><br />Great Camp!!!\n",
            "Sentence in lower case:\n",
            " if you like 100 original gut wrenching laughter you will leaves like this movie. https:www.google.com \\ .<br /><br />great camp!!!\n",
            "_________________________________________________\n",
            "After removing urls:\n",
            " if you like 100 original gut wrenching laughter you will leaves like this movie.  \\ .great camp!!!\n",
            "_________________________________________________\n",
            "After removing numbers:\n",
            " if you like  original gut wrenching laughter you will leaves like this movie.  \\ .great camp!!!\n",
            "_________________________________________________\n",
            "After tokenization:\n",
            " ['if', 'you', 'like', 'original', 'gut', 'wrenching', 'laughter', 'you', 'will', 'leaves', 'like', 'this', 'movie', 'great', 'camp']\n",
            "_________________________________________________\n",
            "After removing stop words:\n",
            " ['like', 'original', 'gut', 'wrenching', 'laughter', 'leaves', 'like', 'movie', 'great', 'camp']\n",
            "_________________________________________________\n",
            "After stemming:\n",
            " ['like', 'origin', 'gut', 'wrench', 'laughter', 'leav', 'like', 'movi', 'great', 'camp']\n",
            "_________________________________________________\n",
            "After lemmatization:\n",
            " ['like', 'original', 'gut', 'wrenching', 'laughter', 'leaf', 'like', 'movie', 'great', 'camp']\n",
            "_________________________________________________\n",
            "like original gut wrenching laughter leaf like movie great camp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCY6sb4pX1-d"
      },
      "source": [
        "## Feature extraction : TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChsTffQXhSM"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "Encoder = LabelEncoder()\n",
        "df['sentiment_le']=Encoder.fit_transform(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tiyj6qr0sQ-q",
        "outputId": "dac99c1d-d74d-4a78-a1e6-130f56ad972e"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleanText</th>\n",
              "      <th>sentiment_le</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>one reviewers mentioned watching episode hooke...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically family little boy jake thinks zombie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter mattei love time money visually stunnin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... sentiment_le\n",
              "0  One of the other reviewers has mentioned that ...  ...            1\n",
              "1  A wonderful little production. <br /><br />The...  ...            1\n",
              "2  I thought this was a wonderful way to spend ti...  ...            1\n",
              "3  Basically there's a family where a little boy ...  ...            0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...            1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkEMyU-XUWIy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleanText'], df['sentiment_le'], test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5mGZM15YZrH"
      },
      "source": [
        "tfidf = TfidfVectorizer(max_features=20000)\n",
        "X_train = tfidf.fit_transform(X_train)\n",
        "X_test = tfidf.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5D0Fawcoxq7"
      },
      "source": [
        "## Classification : SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvgLu928fC2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9c6954-6136-4bd5-84b6-4760e5171b4b"
      },
      "source": [
        "from sklearn.svm import SVC # \"Support Vector Classifier\" \n",
        "\n",
        "clf = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', \n",
        "          coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, \n",
        "          class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr',\n",
        "          break_ties=False, random_state=None) \n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print('SVC')\n",
        "print(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred)) \n",
        "print(\"Classification report\")\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC\n",
            "Accuracy score: 0.42857142857142855\n",
            "Confusion Matrix:\n",
            " [[3 0]\n",
            " [4 0]]\n",
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      1.00      0.60         3\n",
            "           1       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.43         7\n",
            "   macro avg       0.21      0.50      0.30         7\n",
            "weighted avg       0.18      0.43      0.26         7\n",
            "\n"
          ]
        }
      ]
    }
  ]
}